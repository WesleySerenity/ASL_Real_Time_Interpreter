This project is an American Sign Language (ASL) hand sign interpreter that uses a webcam, MediaPipe Hands, and a Random Forest classifier to recognize static ASL alphabet letters (A–Z) along with special signs such as space, delete, and a neutral “nothing” pose. On top of the recognition pipeline, I built a Flask-based web interface with three main pages: a home page that shows live detection, a practice mode that turns recognition into an alphabet game, and a typing mode that lets users spell out words on screen using ASL signs. In practice mode, the app displays a target letter, the user signs that letter, and the system checks correctness using debounced predictions so that one stable sign counts as one attempt; the score increases on correct attempts, and a new random target is chosen. In typing mode, each recognized letter is appended to a text buffer, the space sign inserts a space, and the delete sign removes the last character. To make spaces visually obvious, the typed text is rendered in a “hangman-style” layout, where each character appears above an underline and spaces show as an underline with nothing above it. To handle double letters like “Aaron,” the system uses a neutral-reset mechanism: after a letter is committed, the same letter cannot be committed again until the user briefly returns to a neutral state (either no hand detected or the “nothing” sign), which prevents accidental repeated letters from holding a sign while still allowing intentional double letters.

The source code is organized around a few main components. The core application is in app.py, which sets up the Flask server, loads the trained model.p file (a pickled RandomForest model), manages global state for practice and typing modes, and streams video frames using OpenCV. A data directory holds the image dataset organized into subfolders by class label—for example data/A, data/B, … for alphabet letters, and data/space, data/del, and data/nothing for the special classes. Scripts such as an image collection script (for capturing webcam images into the appropriate data/<label> folders), a dataset-building script (for running MediaPipe Hands on all images and saving normalized landmark coordinates and labels into data.pickle), and a training script (for training the RandomForest model and saving it as model.p) are included in the project. The web pages are implemented using Flask templates stored in a templates folder: home.html for the live detection page, practice.html for the alphabet practice game, and typing.html for the ASL typing mode with hangman-style visualization and hint panels. The project uses Python 3, OpenCV for webcam capture and drawing overlays, MediaPipe for hand landmark detection, NumPy for array operations, scikit-learn for model training and prediction, and Flask for the web server and routing.

To set up and run the project, you first need Python 3 and the necessary libraries. It is recommended to create a virtual environment (for example using python -m venv venv, then activating it) and install the required packages via pip install opencv-python mediapipe numpy scikit-learn flask matplotlib or via a provided requirements.txt file if included. Before running the web app, you either use the provided data.pickle and model.p files or generate them yourself. To generate them, you run the image collection script to capture ASL hand sign images into the data directory, then run the dataset script to process all images with MediaPipe Hands and save the landmark features and labels to data.pickle, and finally run the training script that loads data.pickle, trains a RandomForestClassifier, prints test accuracy, and saves the trained model as model.p. Once the model file exists, you launch the application by running python app.py, which starts the Flask server on http://127.0.0.1:5050/ (or the configured host and port). Visiting the root URL in a browser opens the home page with live sign detection and the alphabet reference; from the dropdown navigation you can access the practice mode to work on matching target letters and tracking score, and the typing mode where you spell out words using signs, use the space and delete gestures to manage spacing and corrections, and rely on the neutral-reset logic to type double letters cleanly. For submission, I will include the full source code (or a link to a GitHub repository if the project is hosted remotely), this readme describing the structure and setup/build instructions, and a one-minute demo video link showing the main features and interactions of the system.
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       
